{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of spell checkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pics.me.me/said-lunch-not-launch-imgflip-con-why-spellcheck-is-critical-30239087.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1975 Steve Johnson of Bell Labs wrote the first version of `spell` in an afternoon, which would later become the standard Unix spelling checker for the English language.\n",
    "\n",
    "``` bash\n",
    "prepare filename |                 # remove formatting commands\n",
    "    translit A-Z a-z |             # map upper to lower case\n",
    "        translit ^a-z @n |         # remove punctuation\n",
    "            sort |                 # put words in alphabetical order\n",
    "                unique |           # remove duplicate words\n",
    "                    common -2 dict # report words not in dictionary\n",
    "```\n",
    "(from programming pearls Jon Bentley ...) `# TODO cite correctly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Tuple, Iterable, Callable\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "from profilehooks import timecall, profile\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard english dictionary file on unix systems `/usr/share/dict/words` is quite big, consisting of 99171 different words taking up 920K of space. This is not a major issue with modern computers that have memory in the gigabytes. The big file size can be attributed to the very basic structure of the dictionary, it is simply a (comprehensive) list of english words which doesn't utilize affix- or prefix analysis.  \n",
    "This means that the list contains many sections like this:  \n",
    "> Abyssinia  \n",
    "> Abyssinia's  \n",
    "> Abyssinian  \n",
    "> Abyssinian's  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timecall\n",
    "def spell(filename: str, words = '/usr/share/dict/words') -> Set[str]:\n",
    "    with open(filename) as f, open(words) as w:\n",
    "        w = set(w.read().lower().split('\\n'))\n",
    "        return set(re.sub(r\"[^a-z]\", \"\\n\", f.read().lower()).split('\\n')) - w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spell` like spelling checker in just 4 lines of python (excl. imports) by utilizing the standard libraries.   \n",
    "Obvious shortcomings like converting `isn't` to `isn` and `t` because punctuation is stripped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  spell (<ipython-input-3-a74301b96c53>:1):\n",
      "    0.599 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5065"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spell('big.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed is actually quite good, compared to the already optimized version of `spell` by Doug McIlroy (written in 1978). That version managed to check a 5,000 word document in **under 30 seconds** (on a VAX-11/750 with 3.125 MHz). The dramatic decrease in spell checking time (30 seconds for 5,000 words compared to ~0.14 seconds for nearly 100,000 words) can be largely attributed to tremendous increase in computing power.\n",
    "\n",
    "### But can we do even better by choosing more *efficient datastructures*?  \n",
    "\n",
    "<div style=\"float:left\">\n",
    "    <img src=\"Trie.svg\">\n",
    "    [Trie.svg](https://de.wikipedia.org/wiki/Trie#/media/File:Trie.svg) created by [nd](https://de.wikipedia.org/wiki/Benutzer:Nd) ([CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/))\n",
    "</div>\n",
    "We can use a **Trie** or \"prefix-tree\", which is an ordered tree data structure, to efficiently store all the words from our dictionary.  \n",
    "The nodes themselves don't carry information about the key they're storing, instead the key is solely encoded in the position of the node and the information associated with the edges inside it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Node object that solely stores information about neighbouring nodes and which letter the the connecting edges represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IllegalTrieEdge(Exception):\n",
    "    pass\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.edges = {}\n",
    "        self.end_of_word = False  # note that not only leaves can be valid words, but also intermediate nodes\n",
    "        \n",
    "    def add_edge(self, ch: str):\n",
    "        if len(ch) != 1:\n",
    "            raise IllegalTrieEdge('Edges can only consist of one character')\n",
    "        if ch not in self.edges:\n",
    "            self.edges[ch] = Node()\n",
    "        return self.edges[ch]\n",
    "    \n",
    "    def get_edge(self, ch: str):\n",
    "        return self.edges.get(ch)\n",
    "    \n",
    "    def __contains__(self, edge: str):\n",
    "        return edge in self.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very basic implementation of a prefix tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie:\n",
    "    def __init__(self, words=[]):\n",
    "        self.root = Node()\n",
    "        self._build_trie(words)\n",
    "        \n",
    "    def _find_last_prefix_node(self, word: str) -> Tuple[str, Node]:\n",
    "        v = self.root\n",
    "        prefix = ''\n",
    "        while len(word) > 0 and word[0] in v:\n",
    "            prefix += word[0]\n",
    "            v = v.get_edge(word[0])\n",
    "            word = word[1:]\n",
    "        return prefix, v\n",
    "    \n",
    "    def _add_word(self, word: str) -> bool:\n",
    "        prefix, node = self._find_last_prefix_node(word)\n",
    "        if prefix == word:\n",
    "            node.end_of_word = True\n",
    "            return False\n",
    "        word = word[len(prefix):]\n",
    "        while len(word) > 0:\n",
    "            node = node.add_edge(word[0])\n",
    "            word = word[1:]\n",
    "        node.end_of_word = True\n",
    "        return True\n",
    "    \n",
    "    def _build_trie(self, words: Set[str]):\n",
    "        for word in words:\n",
    "            self._add_word(word)\n",
    "            \n",
    "    def __contains__(self, word: str) -> bool:\n",
    "        prefix, node = self._find_last_prefix_node(word)\n",
    "        return prefix == word and node.end_of_word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So lets test it, shall we?  \n",
    "As the dictionary to store inside the trie we'll again use the standard english words file on unix systems, splitted at newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/usr/share/dict/words') as w:\n",
    "    word_file_str = w.read()  \n",
    "    words = [word for word in word_file_str.split('\\n') if len(word) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets quickly confirm that the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie = Trie(words)\n",
    "[word for word in words if word not in trie]\n",
    "'adasedadwa' not in trie\n",
    "'Hallo' not in trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory decrease by using a prefix tree (trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size read file: 938664 B; size word list: 6884128 B; size of trie: 93362984 B'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Increase in size by factor 99.46368881729778'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "size_word_file = asizeof.asizeof(word_file_str)\n",
    "size_words = asizeof.asizeof(words)\n",
    "size_trie = asizeof.asizeof(trie)\n",
    "f\"Size read file: {size_word_file} B; size word list: {size_words} B; size of trie: {size_trie} B\"\n",
    "f\"Increase in size by factor {size_trie/size_word_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; margin-right: 20px\">\n",
    "    <img src=\"https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy-downsized.gif\">\n",
    "</div>\n",
    "### An increase by a factor of ~ 100 ?!\n",
    "\n",
    "Python is a highly dynamic language where **everything** is an object, even simple things like an integer or character. This adds an immense overhead, which results in this dramatic increase in size.  \n",
    "So saving space by using a prefix tree implemented in native Python is not possible, due to the limitations of the language.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### But maybe it is at least fast?  \n",
    "\n",
    "We'll adapt the `spell` function from earlier to calculate $A \\backslash B$ where $A = \\{w | w$ in file$\\}$ and \n",
    "$B = \\{w | w$ in dictionary$\\}$ and $B$ is a Python object implementing the `__contains__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timecall\n",
    "def spell_G(filename: str, word_dict) -> Set[str]:\n",
    "    with open(filename) as f:\n",
    "        return {\n",
    "            word for word in\n",
    "            # read the file to check -> convert to lower -> split at newlines \n",
    "            # -> remove punctuation -> remove double occurences (conv. to set)\n",
    "            set(re.sub(r\"[^a-z']\", \"\\n\", f.read().lower()).split('\\n'))\n",
    "            if word not in word_dict\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  spell_G (<ipython-input-75-517a3c4f3e63>:1):\n",
      "    0.776 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7435"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spell_G('big.txt', trie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  spell_G (<ipython-input-75-517a3c4f3e63>:1):\n",
      "    0.558 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7435"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import marisa_trie\n",
    "trie_efficient = marisa_trie.Trie(words)\n",
    "len(spell_G('big.txt', trie_efficient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  spell_G (<ipython-input-75-517a3c4f3e63>:1):\n",
      "    0.551 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7435"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dawg\n",
    "base_dawg = dawg.DAWG(words)\n",
    "len(spell_G('big.txt', base_dawg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance (edit distance)\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\qquad\\operatorname{lev}_{a,b}(i,j) = \\begin{cases}\n",
    "  \\max(i,j) & \\text{ if} \\min(i,j)=0, \\\\\n",
    "  \\min \\begin{cases}\n",
    "          \\operatorname{lev}_{a,b}(i-1,j) + 1 \\\\\n",
    "          \\operatorname{lev}_{a,b}(i,j-1) + 1 \\\\\n",
    "          \\operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)}\n",
    "       \\end{cases} & \\text{ otherwise.}\n",
    "\\end{cases}\n",
    "\\end{equation}  \n",
    "\n",
    "where $1_{(a_i \\neq b_j)}$ is an indicator function equal to 0 when $a_i = b_j$ and equal to 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s: str, t: str):\n",
    "    if not(s and t):    # use falsy value of '' \n",
    "        return len(s) or len(t)\n",
    "    \n",
    "    cost = 0 if s[-1] == t[-1] else 1\n",
    "    \n",
    "    return min(levenshtein_distance(s[:-1], t) +1,\n",
    "               levenshtein_distance(s, t[:-1]) +1,\n",
    "               levenshtein_distance(s[:-1], t[:-1]) + cost\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timecall\n",
    "def close_strings(s: str, words: Iterable[str],\n",
    "                  distace_func: Callable[[str, str], int],\n",
    "                  max_distance: int):\n",
    "    return [word for word in words if 0 < distace_func(s, word) <= max_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  close_strings (<ipython-input-101-fef93a3e73b0>:1):\n",
      "    11.713 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bi', 'Chi', 'Ci', 'Di', 'Li', 'Ni', 'Si', 'Ti', 'chi', 'h']"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_strings('hi', words, levenshtein_distance, 1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def levenshtein_dp(s: str, t: str):\n",
    "    n = len(s) + 1\n",
    "    m = len(t) + 1\n",
    "    d = np.zeros((n, m))\n",
    "    d[:, 0] = range(n)\n",
    "    d[0, :] = range(m)\n",
    "    \n",
    "    for j in range(1, m):\n",
    "        for i in range(1, n):\n",
    "            cost = 0 if s[i-1] == t[j-1] else 1\n",
    "            d[i, j] = min(d[i-1, j] + 1,\n",
    "                          d[i, j-1] + 1,\n",
    "                          d[i-1, j-1] + cost\n",
    "                         )\n",
    "    return d[n-1,m-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Damerau-Levenshtein distance\n",
    "<br>\n",
    "\\begin{equation}\n",
    "d_{a,b}(i,j) = \\begin{cases}\n",
    "  \\max(i,j) & \\text{ if} \\min(i,j)=0, \\\\\n",
    "\\min \\begin{cases}\n",
    "          d_{a,b}(i-1,j) + 1 \\\\\n",
    "          d_{a,b}(i,j-1) + 1 \\\\\n",
    "          d_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)} \\\\\n",
    "          d_{a,b}(i-2,j-2) + 1\n",
    "       \\end{cases} & \\text{ if } i,j > 1 \\text{ and } a_i = b_{j-1} \\text{ and } a_{i-1} = b_j \\\\\n",
    "  \\min \\begin{cases}\n",
    "          d_{a,b}(i-1,j) + 1 \\\\\n",
    "          d_{a,b}(i,j-1) + 1 \\\\\n",
    "          d_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)}\n",
    "       \\end{cases} & \\text{ otherwise.}\n",
    "\\end{cases}\n",
    "\\end{equation}  \n",
    "<br>\n",
    "Extend `levenshtein_dp()` algorithm to include transpositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damerau_levenshtein(s: str, t: str):\n",
    "    n = len(s) + 1\n",
    "    m = len(t) + 1\n",
    "    d = np.zeros((n, m))\n",
    "    d[:, 0] = range(n)\n",
    "    d[0, :] = range(m)\n",
    "    \n",
    "    for j in range(1, m):\n",
    "        for i in range(1, n):\n",
    "            cost = 0 if s[i-1] == t[j-1] else 1\n",
    "            d[i, j] = min(d[i-1, j] + 1,\n",
    "                          d[i, j-1] + 1,\n",
    "                          d[i-1, j-1] + cost\n",
    "                         )\n",
    "            if i > 1 and j > 1 and s[i-1] == t[j-2] and s[i-2] == t[j-1]:\n",
    "                d[i,j] = min(d[i,j], d[i-2, j-2] + cost)\n",
    "    return d[n-1,m-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  close_strings (<ipython-input-101-fef93a3e73b0>:1):\n",
      "    4.639 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out1 = close_strings('ih', words, damerau_levenshtein, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  close_strings (<ipython-input-101-fef93a3e73b0>:1):\n",
      "    4.581 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out2 = close_strings('ih', words, levenshtein_dp, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi'}"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(out1) - set(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('count_1w.txt') as cnt, open('words.txt') as w:\n",
    "    cnt = [cn for cn in cnt.read().split('\\n') if cn]\n",
    "    w = [w for w in w.read().split('\\n') if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  spell (<ipython-input-3-a74301b96c53>:1):\n",
      "    0.740 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spell('big.txt', 'words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dict = {word: 1 for word in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  spell_G (<ipython-input-75-517a3c4f3e63>:1):\n",
      "    0.582 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7189"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spell_G('big.txt', w_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b7\u001b[?47h\u001b[?1h\u001b=\r",
      "the     23135851162\u001b[m\r\n",
      "of      13151942776\u001b[m\r\n",
      "and     12997637966\u001b[m\r\n",
      "to      12136980858\u001b[m\r\n",
      "a       9081174698\u001b[m\r\n",
      "in      8469404971\u001b[m\r\n",
      "for     5933321709\u001b[m\r\n",
      "is      4705743816\u001b[m\r\n",
      "on      3750423199\u001b[m\r\n",
      "that    3400031103\u001b[m\r\n",
      "by      3350048871\u001b[m\r\n",
      "this    3228469771\u001b[m\r\n",
      "with    3183110675\u001b[m\r\n",
      "i       3086225277\u001b[m\r\n",
      "you     2996181025\u001b[m\r\n",
      "it      2813163874\u001b[m\r\n",
      "not     2633487141\u001b[m\r\n",
      "or      2590739907\u001b[m\r\n",
      "be      2398724162\u001b[m\r\n",
      "are     2393614870\u001b[m\r\n",
      "from    2275595356\u001b[m\r\n",
      "at      2272272772\u001b[m\r\n",
      "as      2247431740\u001b[m\r\n",
      "\u001b[7mcount_1w.txt\u001b[m\u001b[K"
     ]
    }
   ],
   "source": [
    "!less count_1w.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_output": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
